---
title: "Relationship between Annual Fire Size Estimates from Quantile Regression and Extreme Value Theory"
author: "Reed Benkendorf"
date: "2025-03-06"
output: pdf_document
---

Quantile regression performs well with estimating the conditional median value within reasonable probabilities, e.g. > 0.8 (from 0.1 to 0.9) to perhaps the 0.9 (0.05 to 0.95) band. 
How accurate these models fits are will depend on the nature, and size, of the data. 
Extreme Value theory is accepted as the preferred method for estimating values in the more distal probabilities. 
We have developed both quantile estimates, and extreme value estimates for fire size, under the current fire regime. 

```{r attach libraries}
library(tidyverse)
library(quantregGrowth)
source('functions.R')
```

However, how well they 'fit together' to characterize extreme values is unknown. 
Under predictions of the extreme values, alongside over predictions of the quantile bands could lead to scenarios where the quantile values *exceed* extreme values. 

```{r read in data}
p <- file.path('..', 'results', 'Tabular')
qf <- file.path(p, 'QuantileForecasts')
fri <- file.path(p, 'FireReturnIntervals')

qfd <- lapply(file.path(qf, list.files(qf, pattern = 'allTau')), read.csv) 
names(qfd) <- gsub('-allTau.*', '', list.files(qf, pattern = 'allTau'))
qfd <- bind_rows(qfd, .id = 'Region') |>
  mutate(Region = str_replace_all(Region, '_', ' '))

frid <- lapply(file.path(fri, list.files(fri,  pattern = 'Predictions')), read.csv) 
names(frid) <- gsub('Predictions-|[.]csv', '', list.files(fri,  pattern = 'Predictions'))
frid <- bind_rows(frid, .id = 'Region')
frid <- rename(frid, return_lvl = X)

fire_sum <- read.csv(
  file.path('..', 'data', 'processed', 'NoFires-TotalArea_byDOIRegion.csv')) %>% 
  rename(Region = REG_NAME)

rm(p, fri, qf)
```

We can focus on using the extreme values only in the upper tails. 
The very low fire areas we will just set as a floor value, of the lowest calculated quantile. 
We are not very concerned about the details down that low - the floors are relatively consistent - the data are skewed towards the ceilings. 
Further in nearly all instances the extreme values in the lower parts of the range are drastically inflated upwards, which would skew our seed demand to being higher than neccessary. 

```{r Subset to relevant records}
frid <- filter(frid, CDF > 0.95) |>
  mutate(Tau = CDF, Approach = 'Extreme') |>
  rename(Prediction = estimate)
```

The following plot is intended for internal use only (i.e. it's really ugly), essentially it is for determining where a 'gap' exists between the 0.95 quantile, and extreme value predictions for that quantile and upwards. 

```{r Plot relationship}

frid <- select(frid, Region, Tau, Prediction, Approach)
qfd <- select(qfd, -FIRE_YEAR) |>
  mutate(Approach = 'Quantile')

ggplot() + 
  geom_point(data = frid, aes(x = Tau, y = Prediction, shape = Approach, color = Approach)) + 
  geom_point(data = qfd, aes(x = Tau, y = Prediction, shape = Approach, color = Approach)) + 
  geom_point(data = fire_sum, aes(y = TotalArea_Acre), x = 0.99, alpha = 0.5) + 
  facet_wrap(~ Region, scales = 'free') + 
  theme_bw() + 
  labs(
    title = 'Relationships between Quantile and Extreme Value predictions',
    y = 'Total Burned Area Prediction (Acres)'
    )

#rm(frid, qfd)
```

The above plots are intended to display the continuity, or lack thereof, between the two methods of near-term fire quantile predictions. 
If the predictions from the extreme value estimates are 'hidden' by the 0.95 quantile prediction, it indicates a flat plateau of total possible burned area size, based on the data fed into these models. 
If the 'extreme' points are above the quantile predictions, it shows that these data and models indicate that a large cumulative burned area may yet to be achieved in reality. 
If the 0.95 quantile forecast exceeds the extreme forecast, that indicates some major limitation of the data. 

For six of these regions (CA-Great Basin, Columbia Plateau PNW, Great Lakes, Lower Colorado Basin, Mississippi Basin, Missouri Basin) the estimates are similar. 
Four of these regions have extreme value predictions which greatly exceed the 0.95 quantile band (Alaska, Arkansas - Rio Grande, Pacific Islands, Upper Colorado Basin).
I am equivocal about which informally described group North Atlantic-Appalachian would fall into. 
South Atlantic Gulf is an instance where the area described by the 0.95 quantiles exceeds the 'extreme' estimates, clearly showing they are limited in their utility. 

In terms of relative difference of predictions, the Pacific Islands display the greatest discordance between predictions, with the extreme values being roughly 4x the 0.95 quantile. 
Note that this data only spans until 2022, missing the 2023 Oahu wildfires which burned a total of ~ 17,000 acres; although not all of this was DOI administered lands. 
The greatest absolute difference between predictions is in Alaska with a roughly 1.5 M discrepancy. 
These differences reflect the amount of land in each region - Hawaii is smallest and Alaska the largest.

## Create Final Estimates Data set for simulations 

We know that the upper limits of the quantile estimates differ from the extreme value theory predictions. 
For any quantiles which exceed the EVT predictions we will replace them with values interpolated from the 'highest' quantile less than an EVT prediction, and the lowest EVT prediction. 

```{r reconcile quantiles and EVT}

dat_new <- lapply(X = split(qfd, f = qfd$Region), FUN = reconcileEVT_quantiles, ev = frid) |>
  bind_rows()

rm(datL)
```

The areas which had interpolation applied to them are evident by how straight the predictions for the upper quantiles are.
In the slightly modified, and still ugly, plot below we have the original values in purple, allowing for a quick visual inspection of where values changed. 

```{r plot difference in overwritten estimates}

ggplot(dat_new, aes(x = Tau, y = Prediction)) + 
  geom_point(data = qfd, aes(shape = Approach), color = 'purple',alpha = 0.8) + 
  geom_point(data = fire_sum, aes(y = TotalArea_Acre), x = 0.99, alpha = 0.5) + 
  geom_point(aes(shape = Approach, color = Approach)) + 
  facet_wrap(~ Region, scales = 'free') + 
  theme_bw() + 
  labs(
    title = 'Relationships between Quantile and Extreme Value predictions',
    y = 'Total Burned Area Prediction (Acres)'
    )

rm(fire_sum)
```

Finally we will also linearly interpolate the growth chart so that we can sample from it using weights, and have a similar number of rows per quantile distance. 

```{r Resample predictions to same nrows per stretch of values}

datL <- split(dat_new, f = dat_new$Region)

all_quants <- function(x){
  
  des_res <- seq(0.01, 0.95, by = 0.0001); des_res <- setdiff(des_res, x$Tau)
  
  dt <- data.frame(
    'Tau' = des_res,
    'Prediction' = approx(c(0, x[['Tau']]), c(0, x[['Prediction']]), xout = des_res)$y,
    'Method' = 'Quantile'
  ) |> 
    dplyr::bind_rows(x) |>
    dplyr::arrange(Tau)
}

datL <- lapply(datL, all_quants) |>
  # fill newly created rows with values before binding if need be. 
  bind_rows()

rm(dat, dat_new)
```

```{r plot quantiles and evt}

ggplot(datL, aes(x = Tau, y = Prediction)) +
  geom_point() +
  facet_wrap(~ Region, scales = 'free') +
  theme_bw() +
  labs(
    title = 'Relationships between Quantile and Extreme Value predictions',
    y = 'Total Burned Area Prediction (Acres)'
    )

write.csv(datL, '../results/Tabular/QuantileForecasts/AllValuesInterpolated.csv', row.names = FALSE)
```



# start trying simulation 

```{r}

x_l <- read.csv(
  file.path('..', 'data', 'processed', 'NoFires-TotalArea_byDOIRegion.csv'))  %>% 
  split(., f = .$REG_NAME)

ak <- x_l[['Mississippi Basin']]

CalculateReturnIntervals(
  x = ak, plot = FALSE, write = FALSE, ret_period = c(95, 100, 0.1), burn.in = 10000)
```

```{r}

library(foreach)
# one set of sims works... 
bnch <- bench::mark(
  outtie <- BurnedAreasSimulator(
    historic = ak, extremes = frid, sims = 10, years = 5, recalc_extremes = 'none') # 'every is failing when many
)


rm(outtie)
# 20 sims = 53 seconds
# 40 sim = 1.75 minutes
# 500 sims = 22.3 minutes  # scales linearly. 

d2 <- data.frame(outtie$Predictions) |>
  rownames_to_column('Year')
dd <- pivot_longer(d2, !Year)

blah <- data.frame(outtie$Tau) |>
  rownames_to_column('Year') |>
  pivot_longer( !Year)
  

hist(dd$value)

# write these out for now...  so no need to re run the sims, looks like 1000 sims is a good target. 
# but that we need to allow for the extreme values to be 'refreshed' at the midpoint of simulation
# e.g. if running for 5 years, re-run the extreme values at year 3. 

ggplot(data = dd, aes(x = Year, y= value)) +
  geom_point(alpha = 0.4)

rm(form.sm)
```


