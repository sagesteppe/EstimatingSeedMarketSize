---
title: "Tabulate Land Cover"
author: "Reed Benkendorf"
date: "2025-01-14"
output: pdf_document
---

For the purposes of the restoration ecologist survey, and fire size simulations, we are interested in tabulating a variety of metrics which relate to the amount of National Land Cover Database (NLCD) cover class area by Department of Interior (DOI) region. 
These metrics are calculated in the script "TabulateTerrestrialAreasbyDOIRegion", while here we focus on cleaning and processing the data sets so that the values of interested can be calculated there without further processing. 
In an effort to make the survey as concise as possible respondents will only interact with combinations of cover class and slopes which make up >90% of the surface area in their regions. 

```{r load libraries, echo = F}
library(terra)
library(tidyverse)
library(sf)
sp <- file.path('..', 'data', 'geospatial', 'NLCD')

antimeridian <- data.frame(
  x = c(-180, -180), 
  y = c(0, 90)
) |>
  sf::st_as_sf(coords = c('x', 'y'), crs = 4326) |>  
  sf::st_union() |> 
  sf::st_cast('LINESTRING') 
```

# 1 Clean NLCD data 

The formats of NLCD data have changed over the years. 
Older data sets utilize '.img' files, these have three bands (red, blue, green) running from 0 to 255 bits, and the three combinations of the bits in each band become the land cover class. 
I assume these are derived from a simple classification of RGB images, and the values not written over into a new raster. 
Newer data sets utilize a '.tif' file format where a single raster band is present; it results the output classifying the three bands to a land cover type. 

The NLCD data products we use span a variety of years, and all were downloaded from https://www.mrlc.gov/ in early January of 2025. 
The product from Alaska is from 2016. 
Because the growing season in Alaska is very short, and it is generally cloudy, it takes multiple years of satellite imagery to obtain a new set of mosaic images which can be used to generate a new cover class feature. 
Hawaii has swapped from a 30m resolution product to a 1m resolution product. 
While this is nice for many applications, it is simply unnecessary for ours, and why the producers do not also offer a more moderate resolution data set is puzzling. 
Because we know that the pacific islands have the fewest fires or our DOI regions, and that no new product has been developed since the 2023 fires, we will use a product at 30m resolution from 2006. 
For Puerto Rico we will use the 2001 data set; this is the only year which is available for Puerto Rico. 
My interpretation of the metadata is that because PR has never had a ground verification campaign to verify the accuracy of this inaugural product, new products have not been made which incorporate the feedback from a ground campaign. 
The data set used for the Continental United States is the most recent, from 2023. 
This data set is updated annually. 

Note that the 'img' data sets are - well antiquated - it's considerably easier to use (`gdal_translate`)[https://gdal.org/en/stable/programs/gdal_translate.html] on the command line to convert them to .tif and then read those into R and reprocess them from there. 
Essentially the `terra` R package offers less support for esoteric file types than it's predecessor the `raster` package, and relies more on `GDAL` to interface with them. 

```{sh convert img files to tif for easy import to rstudio, eval = F}
# note running things on the terminal from Rstdio, never really works well for me... 
# but if you cd into the dir with these data on the real command line, this should
# work no problemo. I think the stand alone GDAL is installed with an install of
# terra in R now. if not, well down the linux rabbit hole you go. I would expect
# the gdal r package to have this support too.... But always feel that isn't superbly
# maintained to the rockstar status stand alone GDAL is. 
cd ~/Documents/EstimatingSeedMarketSize/data/geospatial/NLCD

# simplify the out filenames down to just tif... 
gdal_translate -of GTiff  HI_landcover_wimperv_9-30-08_se5/hi_landcover_wimperv_9-30-08_se5.img HI_landcover_wimperv_9-30-08_se5/HI_NLCD-raw.tif
gdal_translate -of GTiff  NLCD_2016_Land_Cover_AK_20200724/NLCD_2016_Land_Cover_AK_20200724.img NLCD_2016_Land_Cover_AK_20200724/AK_NLCD-raw.tif
gdal_translate -of GTiff  PR_landcover_wimperv_10-28-08_se5/pr_landcover_wimperv_10-28-08_se5.img PR_landcover_wimperv_10-28-08_se5/PR_NLCD-raw.tif
```

```{r Simplify the NLCD data which was converted from img, eval = F}
# we will import these 'raw' tifs and convert them into more general tif formats. 
# each will become a single layer, and 'background' cells in this case ocean 
# will be cast and the info for all the RGB channels will be dropped. 
fp <- file.path(sp,
  c(
    file.path('HI_landcover_wimperv_9-30-08_se5', 'HI_NLCD-raw.tif'),
    file.path('PR_landcover_wimperv_10-28-08_se5', 'PR_NLCD-raw.tif'),
    file.path('NLCD_2016_Land_Cover_AK_20200724', 'AK_NLCD-raw.tif')
  )
)

toTif <- function(x){
  terra::rast(x) |> 
    droplevels() |> 
    as.numeric() |>
    terra::writeRaster(gsub('-raw.tif', '.tif', x))
}

lapply(fp, toTif)
```

The raster sets will now be reduced into two sets, one will cover North America i.e. Alaska, CONUS, and Puerto Rico.
These data will be in the Albers Equal Area Conic projection for North America (epsg 102008). 
The other data set will be for Hawaii, and represent the Pacific Islands region. 

Given the size of the first set, it will be set up as 'tiles', where each individual piece of the raster are written as separate files (tiles) to disk. 
They can then be loaded as a virtual raster tile (VRT) data set. 

```{r Create a large raster template which covers the area to be tiled, eval = F}

pr <- project(
  rast(
    file.path(sp, 'PR_landcover_wimperv_10-28-08_se5', 'PR_NLCD-raw.tif')), "epsg:4326",
    method = 'mode', # small file, threads would likely slow it down. 
    filename = file.path(sp, 'PR_landcover_wimperv_10-28-08_se5', 'PR_NLCD-WGS84.tif')
  )

ak <- project(
  rast(
    file.path(sp, 'NLCD_2016_Land_Cover_AK_20200724', 'AK_NLCD-raw.tif')), "epsg:4326",
    threads = TRUE, method = 'mode',
    filename = file.path(sp, 'NLCD_2016_Land_Cover_AK_20200724', 'AK_NLCD-WGS84.tif')
  )

conus <- project(
  rast(file.path(sp, 'Annual_NLCD_LndCov_2023_CU_C1V0.tif')), "epsg:4326",
  threads = TRUE, method = 'mode',
  filename = file.path(sp, 'CONUS_NLCD-WGS84.tif')
  )


conus <- rast(file.path(sp, 'CONUS_NLCD-WGS84.tif'))
ak <- rast(file.path(sp, 'NLCD_2016_Land_Cover_AK_20200724', 'AK_NLCD-WGS84.tif'))
pr <- rast(file.path(sp, 'PR_landcover_wimperv_10-28-08_se5', 'PR_NLCD-WGS84.tif'))

#' convert an extent into a spatVector
recoverVect <- function(x, epsg){
  
  terra::as.polygons(terra::ext(x), crs = epsg) |> 
    terra::svc() |> 
    terra::vect()
}

exts <- vect(
  lapply(list(conus, ak, pr), recoverVect, 'epsg:4326')
) |> ext()

exts[2]
exts[2] <- -66.5 # alaska is at the edge of the eastern hemisphere too, we'll skip that for now
# we'll use Quoddy point Maine

extend(conus, exts,
       filename = '/media/steppe/hdd/SeedMarketSizeTemplates/NorthAmerica_NLCD-template.tif', 
       overwrite = FALSE, fill = NA)
```

```{r Combine CONUS Alaska and Puerto Rico Rasters onto the template, eval = F}
# now we need to add the values from the puerto rico and alaska rasters to this template
temp <- rast('/media/steppe/hdd/SeedMarketSizeTemplates/NorthAmerica_NLCD-template.tif')

pr <- resample(pr, temp, method = 'mode', threads = TRUE)
temp1 <- cover(
  temp, pr, 
  filename = '/media/steppe/hdd/SeedMarketSizeTemplates/NorthAmerica_NLCD-CONUS-PR.tif')

temp1 <- rast('/media/steppe/hdd/SeedMarketSizeTemplates/NorthAmerica_NLCD-CONUS-PR.tif')
ak <- resample(ak, temp1, method = 'mode', threads = TRUE)
temp2 <- cover(
  temp1, ak, 
  filename = '/media/steppe/hdd/SeedMarketSizeTemplates/NorthAmerica_NLCD-CONUS-FULL.tif')
```

```{r Split the NLCD data set into tiles}
p <- '/media/steppe/hdd/SeedMarketSizeTemplates'

r_co <- rast(file.path(p, 'NorthAmerica_NLCD-CONUS-FULL.tif'))
tiles <- rast(ext(r_co), nrow = 5, ncol = 5)

makeTiles(
  r_co,
  tiles,
  file.path(p, 'tiles/NLCD.tif'),
  na.rm = TRUE
)

```


## Resample slope to NLCD pixels 

To create combinations of NLCD cover classes and slopes we will use Geomorpho90m data set. 
As the name implies these data were generated at 90m (or 3 arc second) resolution, or roughly 1/3 the resolution of the NLCD data set. 
While we could technically compute a new slope data set based on 30m elevation products, this would require a good amount of compute. 
However, the real reason we do not do that are twofold 1) projecting the DEM to a projection which is suitable for this degree in difference of latitude (essentially from the Equator to the North Pole) is very expensive, 2) I am not confident that the results would be different enough to warrant this step. 
We will use the slope 'maximum', which I would estimate is slightly biased to be lower than observed, as a feature when we resample from 90m into 30m resolution. 

```{r Resample slope to NLCD resolution and mask to NLCD extent}
slopeP <- '/media/steppe/hdd/SeedMarketSizeTemplates/slope'
slope <- vrt(file.path(slopeP, list.files(slopeP, recursive = TRUE)))
nlcd <- vrt(list.files(file.path(p, 'tiles'), full.names = TRUE))

slope <- crop(slope, nlcd, filename = file.path(slopeP, 'NAslope.tif'))
resample(slope, nlcd, filename = file.path(slopeP, 'NAslope-resamp.tif'), method = 'max')
mask(
  rast(file.path(slopeP, 'NAslope-resamp.tif')),
  nlcd,
  filename = file.path(slopeP, 'NAslope-resamp-mask.tif')
)

makeTiles(
  rast(file.path(slopeP, 'NAslope-resamp-mask.tif')),
  tiles,
  file.path(p, 'tiles/slope/slope.tif'),
  na.rm = TRUE
)

```

We will use the Protected Areas of the United States Database (PAD-US) to create a `mask' of terrestrial surfaces. 
The areas in each raster which do not overlap public lands will be replaced with 0 - essentially deleted via the masking process. 

# Process Land Administration Surfaces. 

The protected areas of the United States Database ([PADUS](https://www.usgs.gov/programs/gap-analysis-project/science/pad-us-data-overview)) serves as the definitive documentation for all lands administered by the US Federal Agencies. 
The name PADUS is a misnomer, while historically the product focused on lands managed for preservation ('protected'), it was grown to include all government lands. 

```{r Subset PADUS to relevant agencies}
p <- file.path('..', 'data', 'geospatial', 'PADUS4_0Geodatabase', 'PADUS4_0_Geodatabase.gdb')
padus <- sf::st_read(
    dsn = p, quiet = TRUE,
    layer = 'PADUS4_0Fee')  |>
  dplyr::select(Mang_Name, Mang_Type, Loc_Mang, Unit_Nm, IUCN_Cat, SHAPE_Area)  |>
  sf::st_cast('MULTIPOLYGON') 

doi <- filter(padus, Mang_Name %in% c('BLM', 'FWS', 'NPS', 'TRIB', 'USBR'))
st_write(doi, file.path('..', 'data', 'geospatial', 'DOIadministered', 'DOIadministeredAreas.shp'))

rm(padus, doi)
```

Some of the areas in the PADUS database cross the antimeridian, or the line defining the Eastern and Western hemispheres. 
When these polygons are treated in most projections, they become invalid, because the softwares do not 
wrap them around the actual spheroid model of the earth. 
We can simply split them into two pieces on the hemisphere. 
Theoretically we may lose a tiny tiny tiny sliver of land, but this is a marine preserve and will actually be inconsequential for our downstream calculations. 

```{r Subset PAD to terrestrial}

p <- file.path('..', 'data', 'geospatial', 'DOIadministered', 'DOIadministeredAreas.shp')
padus <- st_read(file.path(p), quiet = TRUE) 

antimeridian <- antimeridian |> 
  sf::st_transform(st_crs(padus))

indx <- st_intersects(padus, antimeridian) |> lengths()
x_antimeridian <- padus[which(indx==TRUE),]
padus_1hemisphere <- padus[which(indx==FALSE),] 

pad_split <- lwgeom::st_split(crosses, x_antimeridian) |>
  st_collection_extract('POLYGON')

padus <- bind_rows(padus_1hemisphere, pad_split) |>
  sf::st_make_valid()

any(st_is_valid(padus)==TRUE)

st_write(
  padus,
  file.path('..', 'data', 'geospatial', 'DOIadministered', 'DOIadministeredAreas.shp'),
  append = FALSE)

rm(antimeridian, x_antimeridian, padus_1hemisphere, pad_split, indx)
```


PADUS includes both terrestrial and aquatic, especially deep water, areas. 
These will need to be removed from our analysis as they will not be treated by vascular plant seeding efforts. 

For terrestrial surface we use the [GADM](https://gadm.org/about.html), which has excellent resolution for the CONUS coastlines, islands off Alaska, and the Pacific Islands. 
I have visually compared the GADM and natural earth products and can conclusively state that the GADM product more accurately documents coastal areas. 
However, while GADM does not include coastal areas in it's data, which the TIGER line files do, it still includes portions of the Great Lakes. 
To remove those we will use the highest resolution [NaturalEarth](https://www.naturalearthdata.com/) data set; while I am not a fan of using this data set as it has not been reviewed nor is an official government product, the Great Lakes are relatively topographically simple compared to coastal features, and any distortions will be minimized. 

```{r Subset PADUS to terrestrial areas}

p <- file.path('..', 'data', 'geospatial', 'TerrestrialAreas', 'GADM')

lakes <- rnaturalearth::ne_download(
  category = 'physical', type = 'lakes', scale = 10) |>
  select(name)|>
  sf::st_make_valid() |>
  st_union() |>
  st_make_valid()

# now we will erase the lakes from the land surfaces. 
minorIslands <- st_read(file.path(p, 'gadm41_UMI.gpkg'), quiet = TRUE)
us <- st_read(file.path(p, 'gadm41_USA.gpkg'), quiet = TRUE) |>
  sf::st_make_valid() 
us <- bind_rows(us, minorIslands) |>
  sf::st_make_valid() 

us <- st_difference(us, lakes) |>
  st_transform(4326) |>
  select(COUNTRY) 

st_write(us, file.path(p, 'gadm41_USA-land.gpkg'), append = FALSE)

rm(minorIslands, p, us, lakes)
```

Finally we can subset all DOI administered areas in the PADUS database just those which are terrestrial. 

```{r Subset PAD to terrestrial, eval = F}

p1 <- file.path('..', 'data', 'geospatial', 'DOIadministered')
padus <- st_read(file.path(p1, 'DOIadministeredAreas.shp'))

p2 <- file.path('..', 'data', 'geospatial', 'TerrestrialAreas', 'GADM')
us <- st_read(file.path(p2, 'gadm41_USA-land.gpkg'), quiet = TRUE) |>
  sf::st_transform(st_crs(padus)) |>
  st_union()

padus_terrestrial <- st_intersection(padus, us)

sf::st_write(
  padus_terrestrial, 
  file.path(p1, 'DOIterrestrial.gpkg'), append = FALSE
)
```

We will also make a copy of this data set and save it in an unprojected, or geographic coordinate system. 
This set will be used when we need to make area calculations across large areas, e.g. from Puerto Rico to Alaska. 

```{r Save as geographic CRS, eval = F}
p <- file.path('..', 'data', 'geospatial', 'DOIadministered', 'DOIterrestrial.gpkg')
padus <- st_read(file.path(p), quiet = TRUE) |>
  # we are dealing with areas over significant curvature of the earth
  # and many high latitude areas so will need to calculate our areas on 
  # the geoid
  sf::st_transform(4326) 

# when we do this we find out some of our geometries have a problem... 
indx <- st_is_valid(padus)

# notice we have a ton of multipolygons, what we will find is that it's 
# usually a single polygon within these objects which is the problem
invalid <- padus[which(indx == FALSE),]
padus <- mutate(padus, Valid = indx)

# however, sometimes these polygons are duplicates of another large polygon
# i.e. a refuge may have 5 multipolygons, and three of these contain redundant
# information! This probably relates to some kind of administrative duties which 
# differ across the areas. 

problem_units <- filter(padus, Unit_Nm %in% invalid$Unit_Nm) 
problem_units <- st_make_valid(problem_units)
problem_units <- mutate(problem_units, Valid = st_is_valid(problem_units))

# these units have some more serious issues with their geometries
# we will cast all of them to polygon and work on fixing them one at a time. 
vp_units <- problem_units %>% 
  group_by(Unit_Nm) %>% 
  filter(any(Valid == FALSE))

vp_units <- sf::st_cast(vp_units, 'POLYGON')
vp_units <- sf::st_make_valid(vp_units)

vp_units <- vp_units %>% 
  group_by(Unit_Nm) %>% 
  summarize(geom = st_union(geom))

wenatchee <- vp_units[1,'geom']
tug <- vp_units[2,]
tug <- st_cast(tug, 'POLYGON')

tug <- st_make_valid(tug) 
tug <- st_combine(tug) |> 
  st_make_valid() |> 
  sf::st_as_sf() |>
  rename(geom = x)

vp_units <- bind_cols(
  sf::st_drop_geometry(vp_units), 
  bind_rows(wenatchee, tug)
)

fixed <- problem_units %>% 
  group_by(Unit_Nm) %>% 
  filter(all(Valid == TRUE))

vp_units <- problem_units %>% 
  group_by(Unit_Nm) %>% 
  filter(any(Valid == FALSE) & str_detect(Loc_Mang, 'TOGIAK', negate = TRUE)) |>
  sf::st_drop_geometry() |>
  right_join(vp_units, by = 'Unit_Nm')

# these are all of the areas we did not reprocess. 
padus <- padus %>% 
  group_by(Unit_Nm) |>
  filter(all(Valid==TRUE)) |>
  select(-Valid)

padus <- bind_rows(fixed, vp_units, padus)
# now we suspect that individual units have been duplicated by some other 
# process. We will 

padus[38,] <- sf::st_make_valid(padus[38,])
indx <- sf::st_is_valid(padus)

st_write(padus,
   file.path('..', 'data', 'geospatial', 'DOIadministered', 'DOIterrestrial-geographic.gpkg')
)

rm(vp_units, fixed, tug, wenatchee, problem_units, invalid, indx)
```

```{r}
p <- file.path('..', 'data', 'geospatial', 'DOIadministered')

padus_geo <- st_read(file.path(p, 'DOIterrestrial-geographic.gpkg'))
padus_plan <- st_read(file.path(p, 'DOIterrestrial.gpkg'))

areas2 <- data.frame(
  st_drop_geometry(padus_geo), 
  Area = as.numeric(units::set_units(sf::st_area(padus_geo), ha)*2.4710538146717)
) |> 
  group_by(Mang_Name) |>
  summarize(Total = sum(Area))




fws_is_fck <- filter(padus, Mang_Name == 'FWS')
fck_areas <- st_area(fws_is_fck)

fws_is_fck[ order(fck_areas, decreasing = TRUE), 'Unit_Nm']

anwr <- filter(fws_is_fck, Unit_Nm=='Arctic National Wildlife Refuge') |>
  rownames_to_column()

anwr2 <- sf::st_union(anwr) |>
  st_as_sf() |>
  rownames_to_column()

ggplot() + 
  geom_sf(data = anwr2, aes(fill = rowname), alpha = 0.5) +
  facet_grid(~ rowname)
```


## Compare our area estimates to the official values
I went around various sources to see the amount of land the agencies officially administer. 
Two reports by the Congressional Research Service match the numbers which I am able to calculate, but which differ from widely circulated and un-cited values. 
These are a report from early 2020 (["Federal Land Ownership: Overview and Data"](https://crsreports.congress.gov/product/pdf/r/r42346)), which is comprehensive for the big 5 agencies, and a more recent report from February 2023 (["Federal Lands and Related Resources: Overview and Selected Issues for the 118th Congress"](https://crsreports.congress.gov/product/pdf/R/R43429)) which has virtually identical total values. 
Given the similarity in the reports, and that while federal lands often change management *styles* they rarely increase by millions of acres, we will use the latter report as our baseline.  

```{r investigate mismatch between PADUS database and official products}

p <- file.path('..', 'data', 'geospatial', 'PADUS4_0Geodatabase', 'PADUS4_0_Geodatabase.gdb')
padus <- sf::st_read(
    dsn = p, quiet = TRUE,
    layer = 'PADUS4_0Fee')  |>
  dplyr::select(Mang_Name, Mang_Type, Loc_Mang, Unit_Nm, IUCN_Cat, SHAPE_Area)  |>
  sf::st_cast('MULTIPOLYGON') 

doi <- filter(padus, Mang_Name %in% c('BLM', 'FWS', 'NPS', 'TRIB', 'USBR'))
areas <- as.numeric(sf::st_area(doi))

mod <- lm(areas ~ doi$SHAPE_Area)
summary(mod)$r.squared
plot(doi$SHAPE_Area, areas)
indx <- doi$Mang_Name=='USBR'
official <- sum (sf::st_drop_geometry(doi)[indx,'SHAPE_Area'] )
official * 0.00001 * 2.45705

sum(areas[indx])
blm <- filter(doi, Mang_Name == 'USBR')
blm_geo <- sf::st_transform(blm, 4326)
blm_geo <- st_make_valid(blm)

areas_geo <- as.numeric(sf::st_area(blm_geo))
mod_geo <- lm(areas_geo ~ doi$SHAPE_Area[indx])
summary(mod_geo)$r.squared

areas_planar <- as.numeric(sf::st_area(blm))
mod_plan <- lm(areas_planar ~ doi$SHAPE_Area[indx])
summary(mod_plan)$r.squared

# NPS TRACKS OUR NUMBER IS VIRTUALLY INDISTINGUISHABLE FROM THERE 
# ESTIMATED LAND 84,000,000 and 4,502,644 aquatic 

# BLM WE ARE 1.2% OFF THE VALUES WHICH THE AGENCY SHARES. 
prettyNum(sum(doi$SHAPE_Area[indx]) * 0.0001 * 2.45705, ',')
prettyNum(sum(areas_geo) * 0.0001 * 2.45705, ',')
prettyNum(sum(areas_planar) * 0.0001 * 2.45705, ',')
```

What this indicates is that our calculations match the PAD-US teams calculations. 
Our final values, where remove deep water habitat from these agencies terrestrial land sums may however differ. 

```{r}
data.frame(
  agency = c('BLM', 'FWS', 'NPS'),
  crs_area = c(244391312, 89230772, 79972531),
  pad_est = c(),
  geoid = c(), 
  planar = c()
)
```


# Derive NLCD x Slope tiles

For all natural areas which are administered by the DOI agencies we want to have a single product which contains both the NLCD type, and slope of the area. 

```{r slope categories, eval = F}
p <- '/media/steppe/hdd/SeedMarketSizeTemplates/tiles'
fp <- file.path(p, 'slope')

slope <- file.path(fp, list.files(fp))

for (i in 1:length(slope)){
  
  r <- terra::rast(slope[i])
  r <- ifel(r < 10, 0, 10)
  writeRaster(
    r, 
    file.path(p, 'slope_cat', 
              paste0('slope_cat', gsub('[^0-9]', '', basename(slope[i])), '.tif')
              )
    )
}
```

A product which selectively contains only non-developed, areas is developed. 
Note that this also includes areas used for pasture, but not for intensive agriculture. 

```{r Subset NLCD tp natural areas}

want <- c(31, 41, 42, 43, 51, 52, 71, 72, 73, 74, 81, 90, 95)

p <- '/media/steppe/hdd/SeedMarketSizeTemplates/tiles'
fp <- file.path(p, 'nlcd')

nlcd <- file.path(fp, list.files(fp))

for (i in 1:length(nlcd)){
  
  r <- terra::rast(nlcd[i])
  r <- ifel(r %in% want, r, NA)
  writeRaster(
    r, 
    file.path(p, 'natural_nlcd', 
              paste0('natural_nlcd', gsub('[^0-9]', '', basename(nlcd[i])), '.tif'))
    )
}

```

Finally we will subset a product of the NLCD classes to just areas administered by DOI agencies. 

```{r DOI administered areas}

p <- '/media/steppe/hdd/SeedMarketSizeTemplates/tiles'
fp <- file.path(p, 'natural_nlcd')
nlcd <- file.path(fp, list.files(fp))
nlcd1 <- rast(nlcd[1])

p1 <- file.path('..', 'data', 'geospatial', 'DOIadministered', 'DOIadministeredAreas.shp')
doi_admin <- st_read(file.path(p), quiet = TRUE) 
doi_admin <- vect(doi_admin)
doi_admin <- project(doi_admin, crs(nlcd1))

for (i in 1:length(nlcd)){
  
  r <- terra::rast(nlcd[i])
  r <- terra::mask(r, doi_admin)
  
  writeRaster(
    r, 
    file.path(p, 'nlcd_DOI', 
              paste0('nlcd_DOI', gsub('[^0-9]', '', basename(nlcd[i])), '.tif'))
    )
}

```

Finally both the NLCD classes, and the slope of the areas are combined. 

```{r DOI administered areas}

rcl_df_nlcd <- data.frame(
  level = c(31, 41, 42, 43, 51, 52, 71, 72, 73, 74, 81, 90, 95),
  class = c('Barren Land', 'Deciduous Forest', 'Evergreen Forest', 'Mixed Forest',
  'Dwarf Scrub', 'Shrub/Scrub', 'Grassland/Herbaceous', 'Sedge/Herbaceous', 
  'Lichens', 'Moss', 'Pasture/Hay', 'Woodly Wetlands', 
  'Emergent Herbaceous Wetlands')
)

p <- '/media/steppe/hdd/SeedMarketSizeTemplates/tiles'
fp <- file.path(p, 'nlcd_DOI')
nlcd <- file.path(fp, list.files(fp))

fp <- file.path(p, 'slope_cat')
rcl_df_slope <- data.frame(level = 0:1, cat = c('low', 'high'))
slope_cat <- file.path(fp, list.files(fp))

for (i in 1:length(nlcd)){
  
  nlcd_r <- terra::rast(nlcd[i])
  levels(nlcd_r) <- rcl_df_nlcd
  
  slope_r <- terra::rast(slope_cat[i])
  levels(slope_r) <- rcl_df_slope
  outR <- concats(nlcd_r, slope_r)
  
  writeRaster(
    outR, 
    file.path(p, 'nlcd_slope', 
              paste0('nlcd_slope', gsub('[^0-9]', '', basename(nlcd[i])), '.tif'))
    )
}

```

